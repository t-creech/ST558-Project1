---
title: "Data Processing"
format: html
toc: TRUE
editor_options: 
  chunk_output_type: inline
---

# Packages

Before we get to writing any functions that we use, let's make sure that we load in the necessary packages we need. We can also include some handy code that searches for the package and installs it in the event that it is not already installed on the device that is running the code.

```{R,warning=FALSE}
# We will need the tidyverse and tidycensus packages for this part of the project

# Require will load the package if available and return FALSE if not available by adding the ! we will return TRUE if the package is not available, running the code in the if statement
if(!require(tidyverse)){
    # If package is not available, install it then load it again
    install.packages("tidyverse")
    library(tidyverse)
}

# Does same as above for other packages
if(!require(httr)){
    install.packages("httr")
    library(httr)
}

if(!require(jsonlite)){
    install.packages("jsonlite")
    library(jsonlite)
}

if(!require(dplyr)){
    install.packages("dplyr")
    library(dplyr)
}
```

# Functions
This project tasks us with writing functions to tackle API querying and processing of the data. The easiest way to achieve this is to "build" the functions with multiple helper functions that can be tested individually and then combined together into a larger function that will handle the tasks required. One of the great benefits of using and API to collect the data is that a lot of the processing of the data can be handled on the API end by adjusting the URL that is passed into the `GET() function.

## Helper Functions
First, let's work to get the API working and just getting it to return any data. We can do this by writing a function that we will call "basic_data" that queries the API using a URL from the example list and checking the response code. To help us later, the function will also print the first level of the JSON if the function call is successful, and return TRUE if successful and FALSE if not.

```{R}
# Function that uses the example URL to check status of API to ensure data is being retrieved
response_test <- function(example_url) {
  
  # Save response as variable
  response <- httr::GET(example_url)
  
  # Get the status code of the response
  status <- status_code(response)
  
  # 200 is expected if working so if it is 200 print that it is working and invisibly return the response
  if (status == 200) {
    print("API working as expected")
    str(response, max.level = 1)
    invisible(response)}
  
  # If not working, print as such and return FALSE
  else {
    print(paste("API not working. Status Code:", status))
    return(FALSE)}
}

```

Now that we have built a function that can test whether the API call is working as expected and provides us with some helpful details and functionality, let's dive into some of the further data processing steps. Our first task is going to be to build a helper function that takes the data from the API call and turns it into a tibble. From testing our response_test function (see below), we can see that the data we are interested in is included in the content field, so we will use this field and the functionalities of jsonlite to turn this into a tibble. Upon testing the code, it appears that just taking the parsed data and converting it to a tibble leave the headers in the first row of the tibble. To overcome this, we will specifically identify the headers and the data to build the tibble. We can also go ahead and type convert the data in the tibble to coerce data types.

```{R}
# Function that takes the response from the API call and turns it into a tibble.
json_to_tibble <- function(resp) {
  parsed <- fromJSON(rawToChar(resp$content))
  
  # First row headers so save those as variable
  headers <- parsed[1, , drop = TRUE]
  
  # Rest is data so save that as such
  resp_data <- parsed[-1, , drop = FALSE]
  
  # Convert to tibble, naming columns
  data_tibble <- as_tibble(resp_data)
  names(data_tibble) <- headers
  
  return(data_tibble)
}
  
```


## Helper Tests
First we can test the response function to ensure the API is returning a status code of 200.

```{R}
test_url <- "https://api.census.gov/data/2022/acs/acs1/pums?get=SEX,PWGTP,MAR&SCHL=24"

json <- response_test(test_url)
```
Great, now that we have confirmed the API is returning as expected, we can test our next helper function that takes the json and turns it into a tibble.

```{R}
census_data <- json_to_tibble(json)

census_data
```
Looks like this function is working as well!

## API Functions
Now that we have our necessary helper functions to call the API and process the data, let's work on creating the necessary functions to allow the user to choose the data that is being pulled. We will create two functions. A single year function that pulls data for a single year, and a multi-year function that loops through a list of years provided by the user and combines the data. We will start with the single year function.

### Single Year API Call
This function has quite a bit of validation that is needed to ensure that the inputs of the user are provided as expected. This will likely be the bulk of what is done by this function. The rest will simply be to combine the inputs of the function into a cohesive URL that can be sent to the API to query.
```{R}
single_year_data <- function(year = 2022, num_vars = c("AGEP"), cat_vars = c("SEX"), geography = "State", geo_subset = 15){
  # The bulk of this function will include data validation measures. After the input is validated, we will build the URL
  # We will start with the base URL
  base_url <- "https://api.census.gov/data/"
  # List out possible options for each selection in vectors to validate against
  num_var_options <- c("AGEP", "GASP", "GRPIP", "JWAP", "JWDP", "JWMNP")
  cat_var_options <- c("FER", "HHL", "HISPEED", "JWTRNS", "SCH", "SCHL", "SEX")
  state_options <- c("01", "02", "04", "05", "06", "08", "09", "10", "11", "12", "13", "15", "16",
                     "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29",
                     "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
                     "44", "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56", "72")
  geography_level_option <- c("All", "State", "Region", "Division")
  # Set up clauses
  for_clause <- NULL
  get_clause <- NULL
  # Validate the year to ensure it is in applicable years
  if(!(year %in% 2010:2022)) {
    stop("Please provide a valid year that is between 2010 and 2022, inclusively.")    
  }
  else if (year == 2020) {
    stop("Census data not availble for 2020 due to covid pandemic")
  }
  # Validate that at least one numeric variable passed in
  if(length(num_vars) < 1) {
    stop("Must select at least one numeric variable to be included in num_vars.")
  }
  # Validate numeric variable is one of options
  for(num_var in num_vars) {
    if(!(num_var %in% num_var_options)) {
      stop("Invalid numeric variable selection. Please choose from: AGEP, GASP, GRPIP, JWAP, JWDP, and JWMNP")
    }
  }
  # Validate that at least one categorical variable is included
  if(length(cat_vars) < 1) {
    stop("Must select at least one categorical variable to be included in cat_vars.")
  }
  # Check to see if categorical variable is valid
  for(cat_var in cat_vars) {
    if(!(cat_var %in% cat_var_options)) {
      stop("Invalid categorical variable selection. Please choose from: FER, HHL, HISPEED, JWTRNS, SCH, SCHL, SEX")
    }
    else if (cat_var == "HISPEED" & !(year %in% c(2019, 2022))) {
      cat_vars[cat_vars != "HISPEED"]
      print("Skipping HISPEED as not available")
    }
    else if (cat_var == "JWTRNS"& !(year %in% c(2019, 2022))) {
      cat_vars[cat_vars != "JWTRNS"]
      print("Skipping JWTRNS as not available")
    }
  }
  # Check to see if geography selection was valid
  if(!(geography %in% geography_level_option)) {
    stop("Invalid geography level. Please choose from: All, State, Region, Division")
  }
  # If user selects all, leave for clause as null since none needs to be passed
  if(geography == "All") {
    for_clause <- NULL
  }
  # Build for clause in the event state is selected
  else if(geography == "State") {
    if(identical(geo_subset, "*")) {
      for_clause <- "for=state:*"
    }
    else {
      state_codes <- sprintf("%02d", as.integer(geo_subset))
      if (any(is.na(state_codes))) {
        stop("Geography Subset must be * or state code")
      }
      bad_states <- setdiff(state_codes, state_options)
      if (length(bad_states) == 0) {
        for_clause <- paste0("for=state:", paste(state_codes, collapse = ","))
      }
      else {stop("Invalid state code. Please select from available codes.")}
    }
  }
  # Build for clause in the event region is selected
  else if(geography == "Region") {
    if(identical(geo_subset, "*")) {
      for_clause <- "for=region:*"
    }
    else {
      region_codes <- as.integer(geo_subset)
      if (any(is.na(region_codes))) {
        stop("Geography Subset must be * or region code")
      }
      bad_regions <- setdiff(region_codes, c(1:4, 9))
      if (length(bad_regions) == 0) {
        for_clause <- paste0("for=region:", paste(region_codes, collapse = ","))
      }
      else {stop("Invalid region code. Please select all using * or choose 1-4, or 9")}
    }
  }
  # Build for clause in the event division is selected
  else if(geography == "Division") {
    if(identical(geo_subset, "*")) {
      for_clause <- "for=division:*"
    }
    else {
      division_codes <- as.integer(geo_subset)
      if (any(is.na(division_codes))) {
        stop("Geography Subset must be * or division code")
      }
      bad_divisions <- setdiff(division_codes, c(0:9))
      if (length(bad_divisions) == 0) {
        for_clause <- paste0("for=division:", paste(division_codes, collapse = ","))
      }
      else {stop("Invalid division code. Please select all using * or choose 0-9")}
    }
  }
  # Build URL per API specifications
  variables <- unique(c(num_vars, cat_vars))
  get_clause <- paste0("get=PWGTP,", paste(variables, collapse = ","))
  if (!is.null(for_clause)) {query_url <- paste0(base_url, year, "/acs/acs1/pums?", get_clause, "&", for_clause)}
  else {query_url <- paste0(base_url, year, "/acs/acs1/pums?", get_clause)}
  response <- response_test(query_url)
  requested_data <- json_to_tibble(response)
  
  # Finally, let's add the year to a column, so we know what year we are pulling in
  requested_data <- requested_data |> mutate(year = as.character(year))
  
  return(requested_data)
}

print(single_year_data())
```
### Data Cleaning
Now that we have pulled in the data from the API, a quick inspection of the tibble above will show that not all of the fields are very useful in their current forms. For example, fields that we would expect to be categorical or time variables, are just integers. If we investigate, per API docs, some data for categorical and time variables is passed by the API as integers which are currently maintained as doubles. Therefore, we will need to build necessary tibbles to perform relevant lookups to fix this data. To do this, we will create another function called field mapping that handles this task for us. If we look at the fields we are allowing the use to select. Fields that are integers already and make sense as integers, should be fine to leave after adjusting for N/A. The remainder will need to be parsed and mapped. To make this easier on ourselves, we can use an API call to get the field items and make these their own tibbles! It is important that we only do this for the variables that need to be parsed and mapped through to make sure we do not overwrite the int values.

```{R}
# Possible data fields for numeric data and correct type needed
# "AGEP" (int), "GASP" (int), "GRPIP" (percent), "JWAP" (time), "JWDP" (time), "JWMNP" (int)

# Possible data fields for categorical data and correct type needed
# "FER", "HHL", "HISPEED", "JWTRNS", "SCH", "SCHL", "SEX" : All will become factors

data_clean <- function(data) {
  # First, we can get the column names of the tibble that is passed in
  columns <- names(data)
  # Now we can check set intersections with the list of numeric, time, and categorical data fields to get the fields in each type we need to update
  unique_years <- unique(data$year)
  if (length(unique_years) != 1) {
    stop("Pass a single-year tibble to data_clean().")
  }
  year <- as.integer(unique_years[1])
  columns_to_check <- setdiff(columns, c("year", "PWGTP"))
  num_var_check <- intersect(columns_to_check, c("AGEP", "GASP", "GRPIP", "JWMNP"))
  cat_var_check <- intersect(columns_to_check, c("FER", "HHL", "HISPEED", "JWTRNS", "SCH", "SCHL", "SEX"))
  time_vars_check <- intersect(columns_to_check, c("JWAP", "JWDP"))
  base_url <- "https://api.census.gov/data/"
  # Now we can get jsons with explanations of the variables from the API. We will start with numeric variable. Here we can just change the data type
  for (num_var in num_var_check) {
    data[[num_var]] <- as.integer(data[[num_var]])
  }
  # Now for categorical data, we can get the mappings for the characters we were given and map them using factors
  for (cat_var in cat_var_check) {
    query_url <- paste0(base_url, year, "/acs/acs1/pums/variables/", cat_var, ".json")
    response <- response_test(query_url)
    # Since we expect the data to be returned in a different format, we need to parse this data differently
    parsed <- fromJSON(rawToChar(response$content))
    items <- parsed$values$item
    if (!is.null(items)) {
      codes  <- names(items)
      labels <- unname(unlist(items))
      data[[cat_var]] <- factor(as.character(data[[cat_var]]), levels = codes, labels = labels)
    }
  }
  # Finally, time is a bit more complicated. We must get the times form the API, but we also need to parse them and find the midpoint
  for (time_var in time_vars_check) {
    query_url <- paste0(base_url, year, "/acs/acs1/pums/variables/", time_var, ".json")
    response <- response_test(query_url)
    # Since we expect the data to be returned in a different format, we need to parse this data differently
    parsed <- fromJSON(rawToChar(response$content))
    items <- parsed$values$item
    if (!is.null(items)) {
      windows <- tibble(codes = names(items), label = unname(unlist(items)))
    }
    na_rows <- windows |> filter(grepl("N/A", label, ignore.case = TRUE))
    items_clean <- windows |> filter(!grepl("N/A", label, ignore.case = TRUE))
    items_clean <- items_clean |> separate(col = label, into = c("Start_Time", "End_Time"), sep = " to ") |> 
      mutate(Start_Time = str_trim(Start_Time), End_Time = str_trim(End_Time)) |>
      mutate(
        Start_Time = str_replace_all(Start_Time, "a\\.m\\.", "AM"),
        Start_Time = str_replace_all(Start_Time, "p\\.m\\.", "PM"),
        End_Time = str_replace_all(End_Time, "a\\.m\\.", "AM"),
        End_Time = str_replace_all(End_Time, "p\\.m\\.", "PM"),
      ) |>
      mutate(
        Start_Time_Form = parse_date_time(Start_Time, orders = "I:M p"),
        End_Time_Form = parse_date_time(End_Time, orders = "I:M p"),
        Midpoint = Start_Time_Form + (End_Time_Form - Start_Time_Form) / 2,
        !!time_var := format(Midpoint, "%I:%M %p")
      )
  }
}
```

```{R}
data <- single_year_data(num_vars = c("JWAP", "JWDP"))
```
```{R}
data_clean(data)
```



### Multi Year Function

```{R}
#  ---------- Wrapper function for multiple years----------
multiple_years <- function(years,...) {
  multiple_yr_data <- lapply(years, function(y) {
    res <- single_year_data(y,...)  # call single-year function
# Ensure res is a tibble
if (!("data.frame" %in% class(res))) {
  res <- tibble(value = res)  # or wrap the single value appropriately
}
    # ensure year column exists
    if (!"year" %in% names(res)) {
      res <- res %>% mutate(year = y)
    }
    res
  })

  bind_rows(multiple_yr_data)
}
```

```{R}
# example
y <- 2012:2015
final_tbl <- multiple_years(y,num_vars = c("JWAP", "JWDP"))
print(final_tbl)
```

